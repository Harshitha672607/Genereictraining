name: model training v12
inputs:
  - {name: X_data, type: Dataset, description: "Parquet or CSV file for features (DataFrame)"}
  - {name: y_data, type: Dataset, description: "Parquet or CSV file for target (Series or single-column DataFrame)"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: enbalance, type: String, description: "true/false for class imbalance handling", optional: true, default: "true"}
  - {name: target_column, type: String, description: "Column name inside y data if multiple columns", optional: true, default: ""}
  - {name: preprocess_metadata, type: Data, description: "Optional metadata", optional: true}
  - {name: model_name, type: String, description: "linear | logistic | polynomial | random_forest", optional: true, default: "logistic"}
  - {name: use_scaler, type: String, description: "true/false", optional: true, default: "true"}
  - {name: fit_intercept, type: String, description: "true/false", optional: true, default: "true"}
  - {name: regularization, type: String, description: "none | l1 | l2 | elasticnet", optional: true, default: "none"}
  - {name: C_value, type: Float, description: "Regularization strength", optional: true, default: "1.0"}
  - {name: poly_degree, type: Integer, description: "Degree for polynomial", optional: true, default: "2"}
  - {name: solver, type: String, description: "lbfgs | liblinear | newton-cg | sag | saga", optional: true, default: "lbfgs"}
  - {name: max_iter, type: Integer, description: "Maximum iterations", optional: true, default: "100"}
  - {name: random_state, type: Integer, description: "Random seed", optional: true, default: "48"}
  - {name: n_estimators, type: Integer, description: "Number of trees", optional: true, default: "100"}
  - {name: max_depth, type: Integer, description: "Max tree depth (0=unlimited)", optional: true, default: "0"}
  - {name: min_samples_split, type: Integer, description: "Min samples to split", optional: true, default: "2"}
  - {name: min_samples_leaf, type: Integer, description: "Min samples at leaf", optional: true, default: "1"}
  - {name: max_features, type: String, description: "Features for split", optional: true, default: "auto"}
  - {name: bootstrap, type: String, description: "true/false", optional: true, default: "true"}
  - {name: oob_score, type: String, description: "true/false", optional: true, default: "false"}

outputs:
  - {name: model_pickle, type: Model, description: "Trained model (.pkl)"}
  - {name: metrics_json, type: Data, description: "Training metrics JSON"}
  - {name: model_config, type: Data, description: "Model configuration JSON"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4  # NEEDS TO BE UPDATED
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback
        import pandas as pd, numpy as np, joblib
        from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures
        from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet
        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error, log_loss, roc_auc_score
        from sklearn.utils.class_weight import compute_sample_weight
        from sklearn.model_selection import cross_val_score
        from sklearn.pipeline import Pipeline

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def bool_from_str(s):
            return str(s).strip().lower() in ("1","true","t","yes","y")

        def load_data_file(path):
            """Load DataFrame from either Parquet or CSV file."""
            print(f"[INFO] Loading data file: {path}")
            
            if not os.path.exists(path):
                raise FileNotFoundError(f"Data file not found: {path}")
            
            file_size = os.path.getsize(path)
            print(f"[INFO] File size: {file_size} bytes")
            
            # Try Parquet first
            try:
                df = pd.read_parquet(path)
                print(f"[INFO] Successfully loaded as Parquet: {df.shape}")
                return df
            except:
                # Try CSV with multiple encodings
                encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
                for encoding in encodings:
                    try:
                        df = pd.read_csv(path, encoding=encoding, on_bad_lines='warn')
                        print(f"[INFO] Successfully loaded as CSV with {encoding}: {df.shape}")
                        return df
                    except:
                        continue
                
                # Try without encoding
                try:
                    df = pd.read_csv(path, encoding=None, engine='python', on_bad_lines='warn')
                    print(f"[INFO] Successfully loaded as CSV with default encoding: {df.shape}")
                    return df
                except Exception as e:
                    raise ValueError(f"Cannot read file {path}: {e}")

        def try_load_metadata(path):
            if not path or not os.path.exists(path):
                return None
            try:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return None

        def parse_max_features(max_features_str, n_features):
            max_features_str = str(max_features_str).strip().lower()
            
            if max_features_str == "auto":
                return "sqrt"
            elif max_features_str == "sqrt":
                return "sqrt"
            elif max_features_str == "log2":
                return "log2"
            elif max_features_str.endswith("%"):
                try:
                    percentage = float(max_features_str[:-1]) / 100.0
                    return max(1, int(n_features * percentage))
                except:
                    return "sqrt"
            else:
                try:
                    val = int(max_features_str)
                    return max(1, min(val, n_features))
                except:
                    try:
                        val = float(max_features_str)
                        return max(1, int(n_features * val))
                    except:
                        return "sqrt"

        def determine_task(y, user_specified_task, model_name):
            user_task = str(user_specified_task).strip().lower()
            
            if user_task in ["classification", "regression"]:
                print(f"[INFO] Using user-specified task: {user_task}")
                return user_task
            
            if model_name == "logistic":
                print(f"[INFO] Model 'logistic' detected, forcing classification task")
                return "classification"
            
            y_series = pd.Series(y).astype(str).str.strip()
            unique_values = y_series.nunique()
            
            if unique_values <= 10:
                try:
                    y_numeric = pd.to_numeric(y_series, errors='coerce')
                    if y_numeric.notna().all():
                        if y_numeric.apply(float.is_integer).all() and unique_values <= 10:
                            print(f"[INFO] Detected classification (integer classes)")
                            return "classification"
                except:
                    pass
                
                if unique_values <= 10:
                    print(f"[INFO] Detected classification (string categories)")
                    return "classification"
            
            print(f"[INFO] Detected regression (continuous target)")
            return "regression"

        ap = argparse.ArgumentParser()
        ap.add_argument('--X_data', type=str, required=True)
        ap.add_argument('--y_data', type=str, required=True)
        ap.add_argument('--model_type', type=str, default="classification")
        ap.add_argument('--enbalance', type=str, default="true")
        ap.add_argument('--target_column', type=str, default="")
        ap.add_argument('--preprocess_metadata', type=str, default="")
        ap.add_argument('--model_name', type=str, default="logistic")
        ap.add_argument('--use_scaler', type=str, default="true")
        ap.add_argument('--fit_intercept', type=str, default="true")
        ap.add_argument('--regularization', type=str, default="none")
        ap.add_argument('--C_value', type=float, default=1.0)
        ap.add_argument('--poly_degree', type=int, default=2)
        ap.add_argument('--solver', type=str, default="lbfgs")
        ap.add_argument('--max_iter', type=int, default=100)
        ap.add_argument('--random_state', type=int, default=48)
        ap.add_argument('--n_estimators', type=int, default=100)
        ap.add_argument('--max_depth', type=int, default=0)
        ap.add_argument('--min_samples_split', type=int, default=2)
        ap.add_argument('--min_samples_leaf', type=int, default=1)
        ap.add_argument('--max_features', type=str, default="auto")
        ap.add_argument('--bootstrap', type=str, default="true")
        ap.add_argument('--oob_score', type=str, default="false")
        ap.add_argument('--model_pickle', type=str, required=True)
        ap.add_argument('--metrics_json', type=str, required=True)
        ap.add_argument('--model_config', type=str, required=True)
        args = ap.parse_args()

        try:
            print("="*80)
            print("MODEL TRAINING STARTED")
            print("="*80)

            X = load_data_file(args.X_data)
            print(f"[INFO] X shape: {X.shape}")
            print("[INFO] X columns:", X.columns.tolist())

            y_df = load_data_file(args.y_data)
            print(f"[INFO] y_df shape: {y_df.shape}")
            print("[INFO] y_df columns:", y_df.columns.tolist() if isinstance(y_df, pd.DataFrame) else ["single_column"])
            print("[INFO] Sample y values:", y_df.head(5).values.flatten().tolist()[:5])
            
            # Extract target variable
            if isinstance(y_df, pd.DataFrame):
                if y_df.shape[1] == 1 and not args.target_column:
                    y = y_df.iloc[:, 0].copy()
                    target_col_name = y_df.columns[0]
                    print(f"[INFO] Using single column: {target_col_name}")
                elif args.target_column:
                    target_col_name = args.target_column.strip()
                    if target_col_name not in y_df.columns:
                        raise ValueError(f"Target column '{target_col_name}' not found. Available: {list(y_df.columns)}")
                    y = y_df[target_col_name].copy()
                    print(f"[INFO] Using specified column: {target_col_name}")
                else:
                    raise ValueError(f"y data has {y_df.shape[1]} columns. Provide --target_column. Available: {list(y_df.columns)}")
            else:
                y = pd.Series(y_df).copy()
                target_col_name = "target"
                print(f"[INFO] Using Series as target")
            
            print(f"[INFO] Target variable name: {target_col_name}")
            print(f"[INFO] Target shape: {y.shape}")

            # Align lengths
            if len(X) != len(y):
                print(f"[WARN] Length mismatch: X={len(X)}, y={len(y)}")
                m = min(len(X), len(y))
                X = X.iloc[:m].reset_index(drop=True)
                y = y.iloc[:m].reset_index(drop=True)
                print(f"[INFO] Aligned to {m} samples")

            # Determine task
            model_name = args.model_name.strip().lower()
            task = determine_task(y, args.model_type, model_name)
            print(f"[INFO] Task: {task}")
            print(f"[INFO] Model: {model_name}")

            # Initialize metrics and config
            metrics = {"task": task, "samples": int(len(y)), "features": int(X.shape[1]), "target_column": target_col_name}
            model_config = {
                "model_name": model_name,
                "task": task,
                "random_state": int(args.random_state),
                "target_column": target_col_name,
                "params": {}
            }

            if task == "classification":
                print("[INFO] Preparing for classification...")
                
                # Encode target
                y_series = pd.Series(y).astype(str).str.strip()
                le = LabelEncoder()
                y_encoded = pd.Series(le.fit_transform(y_series), index=y.index)
                label_info = {
                    "classes": le.classes_.tolist(),
                    "mapping": dict(zip(le.classes_, le.transform(le.classes_)))
                }
                print(f"[INFO] Encoded {len(label_info['classes'])} classes")
                
                # Remove any NaN
                valid_mask = y_encoded.notna()
                if not valid_mask.all():
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_encoded = y_encoded[valid_mask].reset_index(drop=True)
                
                if model_name == "logistic":
                    print("[INFO] Training Logistic Regression")
                    
                    class_weight_val = 'balanced' if bool_from_str(args.enbalance) else None
                    penalty = args.regularization if args.regularization != "none" else None
                    solver = args.solver
                    
                    if penalty == 'l1' and solver not in ['liblinear', 'saga']:
                        solver = 'liblinear'
                    elif penalty == 'elasticnet' and solver != 'saga':
                        solver = 'saga'
                    
                    lr_model = LogisticRegression(
                        penalty=penalty,
                        C=float(args.C_value),
                        fit_intercept=bool_from_str(args.fit_intercept),
                        class_weight=class_weight_val,
                        solver=solver,
                        max_iter=int(args.max_iter),
                        random_state=int(args.random_state)
                    )
                    
                    if bool_from_str(args.use_scaler):
                        clf = Pipeline([('scaler', StandardScaler()), ('est', lr_model)])
                    else:
                        clf = lr_model
                        
                    model_config["params"] = {
                        "solver": solver, "C": float(args.C_value), "penalty": penalty,
                        "fit_intercept": bool_from_str(args.fit_intercept), "max_iter": int(args.max_iter),
                        "class_weight": class_weight_val, "use_scaler": bool_from_str(args.use_scaler)
                    }

                elif model_name == "random_forest":
                    print("[INFO] Training Random Forest Classifier")
                    
                    class_weight_val = 'balanced' if bool_from_str(args.enbalance) else None
                    max_features_parsed = parse_max_features(args.max_features, X.shape[1])
                    
                    rf_model = RandomForestClassifier(
                        n_estimators=int(args.n_estimators),
                        max_depth=None if int(args.max_depth) <= 0 else int(args.max_depth),
                        min_samples_split=int(args.min_samples_split),
                        min_samples_leaf=int(args.min_samples_leaf),
                        max_features=max_features_parsed,
                        bootstrap=bool_from_str(args.bootstrap),
                        oob_score=bool_from_str(args.oob_score),
                        class_weight=class_weight_val,
                        random_state=int(args.random_state),
                        n_jobs=-1
                    )
                    
                    clf = rf_model
                    model_config["params"] = {
                        "n_estimators": int(args.n_estimators),
                        "max_depth": None if int(args.max_depth) <= 0 else int(args.max_depth),
                        "min_samples_split": int(args.min_samples_split),
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "max_features": max_features_parsed,
                        "bootstrap": bool_from_str(args.bootstrap),
                        "oob_score": bool_from_str(args.oob_score),
                        "class_weight": class_weight_val
                    }

                else:
                    raise ValueError(f"Unsupported model for classification: {model_name}")

                # Train
                clf.fit(X, y_encoded)
                
                # Predictions and metrics
                y_pred = clf.predict(X)
                metrics.update({
                    "accuracy_train": float(accuracy_score(y_encoded, y_pred)),
                    "precision_train": float(precision_score(y_encoded, y_pred, average='weighted', zero_division=0)),
                    "recall_train": float(recall_score(y_encoded, y_pred, average='weighted', zero_division=0)),
                    "f1_train": float(f1_score(y_encoded, y_pred, average='weighted', zero_division=0)),
                    "model": model_name,
                    "label_info": label_info
                })
                
                model_obj = clf

            elif task == "regression":
                print("[INFO] Preparing for regression...")
                
                # Convert to numeric
                y_num = pd.to_numeric(y, errors="coerce")
                valid_mask = y_num.notna()
                if not valid_mask.all():
                    X = X.loc[valid_mask].reset_index(drop=True)
                    y_num = y_num[valid_mask].reset_index(drop=True)
                
                if model_name == "linear":
                    print("[INFO] Training Linear Regression")
                    
                    if args.regularization == "none":
                        reg = LinearRegression(fit_intercept=bool_from_str(args.fit_intercept))
                    elif args.regularization == "l2":
                        reg = Ridge(alpha=1.0/float(args.C_value) if float(args.C_value) > 0 else 1.0, 
                                  fit_intercept=bool_from_str(args.fit_intercept), 
                                  random_state=int(args.random_state),
                                  max_iter=int(args.max_iter))
                    elif args.regularization == "l1":
                        reg = Lasso(alpha=1.0/float(args.C_value) if float(args.C_value) > 0 else 1.0,
                                  fit_intercept=bool_from_str(args.fit_intercept),
                                  random_state=int(args.random_state),
                                  max_iter=int(args.max_iter))
                    elif args.regularization == "elasticnet":
                        reg = ElasticNet(alpha=1.0/float(args.C_value) if float(args.C_value) > 0 else 1.0,
                                       l1_ratio=0.5,
                                       fit_intercept=bool_from_str(args.fit_intercept),
                                       random_state=int(args.random_state),
                                       max_iter=int(args.max_iter))
                    
                    if bool_from_str(args.use_scaler):
                        reg = Pipeline([('scaler', StandardScaler()), ('est', reg)])
                    
                    model_config["params"] = {
                        "fit_intercept": bool_from_str(args.fit_intercept),
                        "regularization": args.regularization,
                        "C": float(args.C_value),
                        "max_iter": int(args.max_iter),
                        "use_scaler": bool_from_str(args.use_scaler)
                    }

                elif model_name == "polynomial":
                    print("[INFO] Training Polynomial Regression")
                    
                    poly_steps = [('poly', PolynomialFeatures(degree=int(args.poly_degree), include_bias=False))]
                    
                    if bool_from_str(args.use_scaler):
                        poly_steps.append(('scaler', StandardScaler()))
                    
                    if args.regularization == "none":
                        linear_model = LinearRegression(fit_intercept=bool_from_str(args.fit_intercept))
                    elif args.regularization == "l2":
                        linear_model = Ridge(alpha=1.0/float(args.C_value) if float(args.C_value) > 0 else 1.0,
                                           fit_intercept=bool_from_str(args.fit_intercept),
                                           random_state=int(args.random_state),
                                           max_iter=int(args.max_iter))
                    elif args.regularization == "l1":
                        linear_model = Lasso(alpha=1.0/float(args.C_value) if float(args.C_value) > 0 else 1.0,
                                           fit_intercept=bool_from_str(args.fit_intercept),
                                           random_state=int(args.random_state),
                                           max_iter=int(args.max_iter))
                    elif args.regularization == "elasticnet":
                        linear_model = ElasticNet(alpha=1.0/float(args.C_value) if float(args.C_value) > 0 else 1.0,
                                                l1_ratio=0.5,
                                                fit_intercept=bool_from_str(args.fit_intercept),
                                                random_state=int(args.random_state),
                                                max_iter=int(args.max_iter))
                    
                    poly_steps.append(('linear', linear_model))
                    reg = Pipeline(poly_steps)
                    
                    model_config["params"] = {
                        "degree": int(args.poly_degree),
                        "fit_intercept": bool_from_str(args.fit_intercept),
                        "regularization": args.regularization,
                        "C": float(args.C_value),
                        "max_iter": int(args.max_iter),
                        "use_scaler": bool_from_str(args.use_scaler)
                    }

                elif model_name == "random_forest":
                    print("[INFO] Training Random Forest Regressor")
                    
                    max_features_parsed = parse_max_features(args.max_features, X.shape[1])
                    
                    rf_model = RandomForestRegressor(
                        n_estimators=int(args.n_estimators),
                        max_depth=None if int(args.max_depth) <= 0 else int(args.max_depth),
                        min_samples_split=int(args.min_samples_split),
                        min_samples_leaf=int(args.min_samples_leaf),
                        max_features=max_features_parsed,
                        bootstrap=bool_from_str(args.bootstrap),
                        oob_score=bool_from_str(args.oob_score),
                        random_state=int(args.random_state),
                        n_jobs=-1
                    )
                    
                    reg = rf_model
                    model_config["params"] = {
                        "n_estimators": int(args.n_estimators),
                        "max_depth": None if int(args.max_depth) <= 0 else int(args.max_depth),
                        "min_samples_split": int(args.min_samples_split),
                        "min_samples_leaf": int(args.min_samples_leaf),
                        "max_features": max_features_parsed,
                        "bootstrap": bool_from_str(args.bootstrap),
                        "oob_score": bool_from_str(args.oob_score)
                    }

                else:
                    raise ValueError(f"Unsupported model for regression: {model_name}")

                # Train
                reg.fit(X, y_num)
                
                # Predictions and metrics
                y_pred = reg.predict(X)
                metrics.update({
                    "r2_train": float(r2_score(y_num, y_pred)),
                    "rmse_train": float(np.sqrt(mean_squared_error(y_num, y_pred))),
                    "mae_train": float(mean_absolute_error(y_num, y_pred)),
                    "mse_train": float(mean_squared_error(y_num, y_pred)),
                    "model": model_name
                })
                
                model_obj = reg

            else:
                raise ValueError(f"Unsupported task: {task}")

            # Save outputs
            ensure_dir_for(args.model_pickle)
            joblib.dump(model_obj, args.model_pickle)

            ensure_dir_for(args.metrics_json)
            with open(args.metrics_json, "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)

            ensure_dir_for(args.model_config)
            with open(args.model_config, "w", encoding="utf-8") as f:
                json.dump(model_config, f, indent=2, ensure_ascii=False)

            print("="*80)
            print("SUCCESS: Training completed")
            print("="*80)

        except Exception as e:
            print("="*80, file=sys.stderr)
            print("ERROR DURING TRAINING", file=sys.stderr)
            traceback.print_exc()
            print("="*80, file=sys.stderr)
            sys.exit(1)

    args:
      - --X_data
      - {inputPath: X_data}
      - --y_data
      - {inputPath: y_data}
      - --model_type
      - {inputValue: model_type}
      - --enbalance
      - {inputValue: enbalance}
      - --target_column
      - {inputValue: target_column}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --model_name
      - {inputValue: model_name}
      - --use_scaler
      - {inputValue: use_scaler}
      - --fit_intercept
      - {inputValue: fit_intercept}
      - --regularization
      - {inputValue: regularization}
      - --C_value
      - {inputValue: C_value}
      - --poly_degree
      - {inputValue: poly_degree}
      - --solver
      - {inputValue: solver}
      - --max_iter
      - {inputValue: max_iter}
      - --random_state
      - {inputValue: random_state}
      - --n_estimators
      - {inputValue: n_estimators}
      - --max_depth
      - {inputValue: max_depth}
      - --min_samples_split
      - {inputValue: min_samples_split}
      - --min_samples_leaf
      - {inputValue: min_samples_leaf}
      - --max_features
      - {inputValue: max_features}
      - --bootstrap
      - {inputValue: bootstrap}
      - --oob_score
      - {inputValue: oob_score}
      - --model_pickle
      - {outputPath: model_pickle}
      - --metrics_json
      - {outputPath: metrics_json}
      - --model_config
      - {outputPath: model_config}
